{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os, warnings\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "from dask_ml.decomposition import PCA\n",
    "from joblib import dump, load\n",
    "\n",
    "import keras\n",
    "from keras.layers.core import Dropout\n",
    "from keras.models import load_model\n",
    "\n",
    "import geopandas\n",
    "from rasterio import features\n",
    "from affine import Affine\n",
    "\n",
    "import dask\n",
    "dask.config.set(scheduler='multiprocessing')\n",
    "\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/srvx11/lehre/users/a1254888/.conda/envs/ml_flood/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfloat(f):\n",
    "    return str(float(f))\n",
    "def sint(i):\n",
    "    return str(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glofas_danube():\n",
    "    glofas = xr.open_dataset('../data/danube/glofas_reanalysis_danube_1981-2002.nc')\n",
    "    glofas = glofas.rename({'lat': 'latitude', 'lon': 'longitude'})  # to have the same name like in era5\n",
    "    glofas = shift_time(glofas, -dt.timedelta(days=1))  # the discharge is the mean of the previous 24h of the timestamp\n",
    "    return glofas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def shift_time(ds, value):\n",
    "    ds.coords['time'].values = pd.to_datetime(ds.coords['time'].values) + value\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_riverpoints(dis):\n",
    "    return (dis > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_of_basin(da, kw_basins='Danube'):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "        da : xr.DataArray\n",
    "            contains the coordinates\n",
    "        kw_basins : str\n",
    "            identifier of the basin in the basins dataset\n",
    "    \"\"\"\n",
    "    def transform_from_latlon(lat, lon):\n",
    "        lat = np.asarray(lat)\n",
    "        lon = np.asarray(lon)\n",
    "        trans = Affine.translation(lon[0], lat[0])\n",
    "        scale = Affine.scale(lon[1] - lon[0], lat[1] - lat[0])\n",
    "        return trans * scale\n",
    "\n",
    "    def rasterize(shapes, coords, fill=np.nan, **kwargs):\n",
    "        \"\"\"Rasterize a list of (geometry, fill_value) tuples onto the given\n",
    "        xray coordinates. This only works for 1d latitude and longitude\n",
    "        arrays.\n",
    "        \"\"\"\n",
    "        transform = transform_from_latlon(coords['latitude'], coords['longitude'])\n",
    "        out_shape = (len(coords['latitude']), len(coords['longitude']))\n",
    "        raster = features.rasterize(shapes, out_shape=out_shape,\n",
    "                                    fill=fill, transform=transform,\n",
    "                                    dtype=float, **kwargs)\n",
    "        return xr.DataArray(raster, coords=coords, dims=('latitude', 'longitude'))\n",
    "    \n",
    "    # this shapefile is from natural earth data\n",
    "    # http://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-1-states-provinces/\n",
    "    shp2 = '/raid/home/srvx7/lehre/users/a1303583/ipython/ml_flood/data/drainage_basins/Major_Basins_of_the_World.shp'\n",
    "    basins = geopandas.read_file(shp2)\n",
    "#    print(basins)\n",
    "    single_basin = basins.query(\"NAME == '\"+kw_basins+\"'\").reset_index(drop=True)\n",
    "#    print(single_basin)\n",
    "    shapes = [(shape, n) for n, shape in enumerate(single_basin.geometry)]\n",
    "\n",
    "    da['basins'] = rasterize(shapes, da.coords)\n",
    "    da = da.basins == 0\n",
    "    return da.drop('basins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_upstream(is_river, lat, lon, basin='Danube'):\n",
    "    \n",
    "    \n",
    "    # longitude condition\n",
    "    is_west = (~np.isnan(is_river.where(is_river.longitude <= lon))).astype(bool)\n",
    "    \n",
    "    mask_basin = get_mask_of_basin(is_river, kw_basins=basin)\n",
    "\n",
    "    nearby_mask = is_river*0.\n",
    "    nearby_mask.loc[dict(latitude=slice(lat+1.5, lat-1.5), \n",
    "                         longitude=slice(lon-1.5, lon+1.5))] = 1.\n",
    "    nearby_mask = nearby_mask.astype(bool)\n",
    "    \n",
    "    mask = mask_basin & nearby_mask & is_west #mask_box_mean_greater & \n",
    "    if 'basins' in mask.coords:\n",
    "        mask = mask.drop('basins')\n",
    "    if 'time' in mask.coords:\n",
    "        mask = mask.drop('time')  # time and basins dimension make no sense here\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shifted_predictors(ds, shifts, variables='all'):\n",
    "    \"\"\"Adds additional variables to an array which are shifted in time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "    shifts : list of integers\n",
    "    variables : str or list\n",
    "    \"\"\"\n",
    "    if variables == 'all': \n",
    "        variables = ds.data_vars\n",
    "        \n",
    "    for var in variables:\n",
    "        for i in shifts:\n",
    "            if i == 0: continue  # makes no sense to shift by zero\n",
    "            newvar = var+'-'+str(i)\n",
    "            ds[newvar] = ds[var].shift(time=i)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reshape_flowmodel(X_dis, y_dis):\n",
    "    \"\"\"Reshape, merge predictor/predictand in time, drop nans.\"\"\"\n",
    "    X_dis = X_dis.to_array(dim='time_feature')  \n",
    "    #print('X before feature-stacking', X_dis)\n",
    "    X_dis = X_dis.stack(features=['latitude', 'longitude', 'time_feature'])\n",
    "    #print('X before featuredrop', X_dis)\n",
    "    Xar = X_dis.dropna('features', how='all')\n",
    "    \n",
    "    yar = y_dis\n",
    "    yar = yar.drop(['latitude', 'longitude'])\n",
    "    yar.coords['features'] = 'dis'\n",
    "    \n",
    "    #print('X, y before concat for time nan dropping', Xar, yar)\n",
    "    Xy = xr.concat([Xar, yar], dim='features')\n",
    "    Xyt = Xy.dropna('time', how='any')  # drop them as we cannot train on nan values\n",
    "    time = Xyt.time\n",
    "    \n",
    "    Xda = Xyt[:,:-1]\n",
    "    yda = Xyt[:,-1]\n",
    "    return Xda, yda, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux.floodmodels import FlowModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = xr.open_dataset('../data/danube/era5_slt_z_slor_lsm_stationary_field.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# era5 = xr.open_dataset('../data/usa/era5_lsp_cp_1981-2017_daysum.nc')\n",
    "# era5 = shift_time(era5, -dt.timedelta(hours=23))\n",
    "\n",
    "era5 = xr.open_dataset('../data/danube/era5_danube_pressure_and_single_levels.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glofas = read_glofas_danube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glofas = glofas.isel(time=slice(0, 365*15))  # just to reduce the amount of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tp' in era5:\n",
    "    tp = era5['tp']*1000\n",
    "else:\n",
    "    tp = (era5['cp']+era5['lsp'])*1000\n",
    "tp.name = 'total precip [mm]'\n",
    "tp = tp.interp(latitude=glofas.latitude,\n",
    "               longitude=glofas.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(d):\n",
    "    if not os.path.isdir(d):\n",
    "        os.makedirs(d)\n",
    "        \n",
    "def replace(string: str, old_new: dict):\n",
    "    for o, n in old_new.items(): \n",
    "        string = string.replace(o, str(n))\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare training data for the FlowModel\n",
    "### features = discharge upstream (t-1, ... t-3)\n",
    "### target = discharge (t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = range(1,4)\n",
    "X = add_shifted_predictors(glofas, shifts, variables='all')\n",
    "X = X.drop('dis')  # current dis is to be predicted, is not a feature\n",
    "\n",
    "y = glofas['dis']  # just this variable as dataarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for rgp in riverpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = dict(time=slice(None, '1990'))\n",
    "N_valid = dict(time=slice('1990', '1995'))\n",
    "\n",
    "\n",
    "# kind, lat, lon will be replaced!\n",
    "ff_mod = '../models/flowmodel/danube/kind/point_lat_lon_flowmodel.pkl'\n",
    "ff_hist = '../models/flowmodel/danube/kind/point_lat_lon_history.png'\n",
    "ff_valid = '../models/flowmodel/danube/kind/point_lat_lon_validation.png'\n",
    "ff_upstream = '../models/flowmodel/danube/kind/point_lat_lon_upstream.png'\n",
    "\n",
    "\n",
    "#model = FlowModel('Ridge', dict(alphas=np.logspace(-3, 2, 6)))\n",
    "\n",
    "model = FlowModel('neural_net', dict(epochs=1000, \n",
    "                                     #filepath=filepath,\n",
    "                                      ))\n",
    "pipe = Pipeline([#('scaler', StandardScaler()),\n",
    "                 #('pca', PCA(n_components=6)),\n",
    "                 ('model', model),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "syncr = dask.config.get('scheduler') == 'synchronous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "@delayed\n",
    "def train_flowmodel(lat, lon):    \n",
    "    global ff_mod, ff_hist, ff_valid, ff_upstream, is_river\n",
    "    \n",
    "    f_mod = replace(ff_mod, dict(lat=lat, lon=lon, kind=model.kind))\n",
    "    if os.path.isfile(f_mod):\n",
    "        return  # dont go any further\n",
    "    \n",
    "    f_hist = replace(ff_hist, dict(lat=lat, lon=lon, kind=model.kind))\n",
    "    f_valid = replace(ff_valid, dict(lat=lat, lon=lon, kind=model.kind))\n",
    "    f_upstream = replace(ff_upstream, dict(lat=lat, lon=lon, kind=model.kind))\n",
    "\n",
    "    upstream = select_upstream(is_river, lat, lon, basin='Danube')\n",
    "    N_upstream = int(upstream.sum())\n",
    "    if syncr: print(N_upstream)\n",
    "    if N_upstream <= 5:\n",
    "        if syncr: \n",
    "            print(lats, lons, 'is spring.')\n",
    "        #mask_springs.loc[dict(latitude=lat, longitude=lon)] = 1.\n",
    "\n",
    "        #plt.imshow(mask_springs.astype(int))\n",
    "        #plt.title('springs')\n",
    "        #plt.show()\n",
    "    else:\n",
    "        if os.path.isfile(f_mod):\n",
    "            if syncr: \n",
    "                print('already trained.')\n",
    "        else:\n",
    "            if syncr:\n",
    "                print(lats, lons, 'is danube river -> train flowmodel')\n",
    "\n",
    "            try:\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.imshow(upstream.astype(int))\n",
    "                plt.title(str(N_upstream)+' upstream points for '+str(lat)+' '+str(lon))\n",
    "                fig.savefig(f_upstream); plt.close('all')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            tp_box = tp.sel(latitude=slice(lat+1.5, lat-1.5), \n",
    "                            longitude=slice(lon-1.5, lon+1.5))\n",
    "            noprecip = tp_box.mean(['longitude', 'latitude']) < 0.1\n",
    "\n",
    "            Xt = X.copy()\n",
    "            yt = y.copy()\n",
    "            \n",
    "            Xt = Xt.where(noprecip, drop=True)\n",
    "            Xt = Xt.where(upstream, drop=True)\n",
    "            yt = yt.sel(latitude=float(lat), longitude=float(lon))\n",
    "            Xda, yda, time = preprocess_reshape_flowmodel(Xt, yt)\n",
    "\n",
    "            X_train = Xda.loc[N_train] \n",
    "            y_train = yda.loc[N_train] \n",
    "            X_valid = Xda.loc[N_valid] \n",
    "            y_valid = yda.loc[N_valid] \n",
    "\n",
    "            if syncr:\n",
    "                print(X_train.shape, y_train.shape)\n",
    "                print(X_valid.shape, y_valid.shape)\n",
    "            ppipe = clone(pipe) \n",
    "            history = ppipe.fit(X_train.values, y_train.values,\n",
    "                               model__validation_data=(X_valid.values, \n",
    "                                                       y_valid.values)) \n",
    "\n",
    "            mkdir(os.path.dirname(f_mod))\n",
    "            dump(ppipe, f_mod)\n",
    "\n",
    "            try:\n",
    "                h = history.named_steps['model'].m.model.history\n",
    "\n",
    "                # Plot training & validation loss value\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.plot(h.history['loss'], label='loss')\n",
    "                ax.plot(h.history['val_loss'], label='val_loss')\n",
    "                plt.title('Model loss')\n",
    "                ax.set_ylabel('Loss')\n",
    "                ax.set_xlabel('Epoch')\n",
    "                plt.legend() #['Train', 'Test'], loc='upper left')\n",
    "                ax.set_yscale('log')\n",
    "                fig.savefig(f_hist); plt.close('all')\n",
    "            except Exception as e:\n",
    "                warnings.warn(str(e))\n",
    "\n",
    "            ppipe = load(f_mod)\n",
    "            y_m = ppipe.predict(X_valid) \n",
    "\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(10,4))\n",
    "                y_m.to_pandas().plot(ax=ax)\n",
    "                y_valid.name = 'reanalysis'\n",
    "                y_valid.to_pandas().plot(ax=ax)\n",
    "                plt.legend()\n",
    "                fig.savefig(f_valid); plt.close('all')\n",
    "            except Exception as e:\n",
    "                warnings.warn(str(e))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAB8CAYAAABwrOvwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPtklEQVR4nO3dW8wd1XnG8f8Tm0CcyAqOgTq2E9PEEAjiUFlAm6q1ChVOiuKqUiS7obJUq9ykKkGRiilS095UqRLRctE2chMCajmIOjRYKIkxTqKqFxxMG1EcY+wmqXFwsGlKOTgFHN5e7Pna7Y99mPPM2t/zk6xv75nZa9Y7e/bymrXWrFFEYGZm6Xlb1xkwM7NyXICbmSXKBbiZWaJcgJuZJcoFuJlZolyAm5klygW4mVmiXIBbkiTtk7S+prTeJ+kVSYvqSM+sLfKNPGZmaXIN3BY0SYtTTNsMXIBboiT9UNLVki6XtFfSS5Kel3TrlM+tkRSStko6DHxraNliSZsk7Z33mRsl7cxeny7pC5IOZ/v7oqR3ZOvWSzoi6SZJPwa+0lT8ZuAC3NJ3G3BbRCwFPgDcl/NzvwpcAFwzb/lO4HxJa4eW/TZwd/b6z4HzgEuBDwIrgT8e2vbngGXA+4Hr84dhVpwLcEvdG8AHJS2PiFci4pGcn/uTiHg1In46vDAiTgAPAJsBsoL8Q8BOSQJ+D7gxIn4SES8DfwZsGkriTeCzEfHa/LTN6uYC3FK3lUGN+GlJj0u6Nufnnp2w7m6yApxB7ftrWcF+FrAEeELSi5JeBL6ZLZ9zPCL+p1AEZiW5k8WSFhEHgc2S3gb8FrBD0nsi4tVpH52w7iFguaRLGRTkN2bLXwB+Cnw4In5UIl2zWrkGbkmTdJ2ksyLiTeDFbPHPqqQZESeBHcDnGbRn786Wvwn8LfAXks7O9r9S0vx2dLNWuAC31G0A9kl6hUGH5qaamjDuBq4G/iEr0OfcBBwCHpH0EvAwcH4N+zMrzDfymJklyjVwM7NEVSrAJW2QdEDSIUnb6sqUWRWSPpnNbTL/376u82ZWp9JNKNnEP88Avw4cAR4HNkfE9+rLnpmZjVOlBn45cCgivh8RrwP3AhvryZaZmU1TZRz4Sk69GeIIcMWkDyxftijWrD7tlGXPPLmkQhbgvItPjF03Ku3h7ZvY96Q0x+17bvm0/BTdbpqq8c+Sacds2rk0jY91v9RVDhQpA6r8Ll/mv16IiLPmL69SgGvEsre0x0i6nmxOiPetXMxju1afsv6a915aIQuwa9d3x64blfbw9k3se1Ka4/Y9t3xafopuN03V+GfJtGM27Vyaxse6X+oqB4qUAVV+lw/Hjv8YtW2VAvwIMFwarwKem79RRGwHtgMs1bJo6kTu4gcyt89dz+X/IedNp0o8eT9b1/5mTd5jMfU/0qHjW+UcmYXvpi/n2lw+6srDyP/Yx8RadJ/D6SxaMXqbKm3gjwNrJZ0r6e0MJvTZWSE9MzMroHQNPCJOSvp9YBewCLg9IloZplXlf/NTmi4aqBXUVdOqu6aQd39N77MPRn1HbcVcZD9z+VxI303Tmjx+zdbuD43cptJkVhHxdeDrVdIwM7NyfCemmVmikpxOtlKvcQOXo2XSmZaPujpI7a2aboao61J6lppLxjXXTdrOpnMN3MwsUa3ORrhUy+IKXdXa/qZZ6J1DXcY/a8d+1uJpW96rlmlXpLN67B+OHU9ExLr5y10DNzNLlAtwM7NEJdmJ2YS8HSu+VDarX95Oe//mTuUauJlZolyAm5klyk0omUmT0oxrNmnjdvcmpZrvNpX5jlM/L/piVo9fnc2wroGbmSWq8xp4lxMLTTMtH3XfLdmXuJtU5li1fY6M2l+RfNc9PfBC0uWAgSY6UJs+d10DNzNLlAtwM7NEtdqEct7FJwo/Aq1ubV2Wlen0mlWjjnmRp9VMe+rJpO3KaCIdd2z2S5nzp8zvtOnv2zVwM7NEddaJOas1kTJxpXosylzN5K3F5O1Anpa2a8E2rMo5kPf5l1WfhVokb1Nr4JJul3RM0lNDy5ZJ2i3pYPb3zMK5NDOzSvI0odwBbJi3bBuwJyLWAnuy92Zm1qJc84FLWgM8GBEXZe8PAOsj4qikFcB3IuL8aemsu+SMeGzXaqC7cZ1tdS4utMv0osdq3PGpu3OyL2Ox3XyTT9MPGk/1+Nc9H/g5EXEUIPt7dpXMmZlZcY2PQpF0vaS9kvYe/8+fNb07M7MFo+wolOclrRhqQjk2bsOI2A5sh8Ej1fp+CVP1cqtvY7rbunSv6/bxvKNL8qbTh2aTrvPRhnHf0aS4mxjL3+epOZpQtga+E9iSvd4CPFBPdszMLK+pNXBJ9wDrgeWSjgCfBT4H3CdpK3AY+ESTmZwk75N0Rm0/av2sdpy0dWXQRKxFa3HDy/tw7BeCaZ3STY7H78tvrYunCU0twCNi85hV/Xm8vJnZAuRb6c3MEtX5fOB1yXt50kVnXl/1sXmhrvHkk9Jr4nI+he+7CwtxbvQ243MN3MwsUb2qgadWiynagdrGfvPko8vj3HZHT974ywyDK7OdpaXqb61proGbmSXKBbiZWaJ61YRSZk7pujtJ+nbJ1Lf8jFN352OTinRiLrQ7++rW5Hj8VNOuk2vgZmaJcgFuZpaoXjWhjFLmEVxlLn/6dqk8KYYmmouqTtw1afx73y5Di8RdZhRPV/E2Pbqob99j3VIbBQeugZuZJauXNfC8Nbsi6cxPb5y+jd8edSyarC3n+fz8dIbXNzFBUd2q5rHM3Z9505l2dVDm3oO67zYtkk6qd2KmkEdwDdzMLFkuwM3MEtXLJpRhk+YTHqeuJ8HUpcsxpWXmY87b9NTW2Pu8TT5tHd8y+ShzHvetQ7iuidrqiivV5pk6uQZuZpaoXtXAqwwZHLe+DlVrHFXy02RHYZF9t6Vo7bXpPFaZsKzMlUWX33deVSdIq3vK5bqO2bR0+nZFBDlq4JJWS/q2pP2S9km6IVu+TNJuSQezv2c2n10zM5uTpwnlJPCZiLgAuBL4lKQLgW3AnohYC+zJ3puZWUvyPBPzKHA0e/2ypP3ASmAjg4cdA9wJfAe4qY5MFekwKzoBVpGOu6L7aELfOmjaugu0asdm0bzVOWlYE00DfUhnTtWmhLzzsfehY7Nvv7/5CnViSloDXAY8CpyTFe5zhfzZYz5zvaS9kva+wWvVcmtmZv8ndwEu6V3AV4FPR8RLeT8XEdsjYl1ErDuN08vk0czMRsg1CkXSaQwK77si4v5s8fOSVkTEUUkrgGNNZLCtW7T7eHkEzYxKaHL8bBO3wE/KW5Fx1WX2l/eYlxk7P2m7Pqr7/BuV9rRlfWvO7LqJJc8oFAFfBvZHxK1Dq3YCW7LXW4AH6s+emZmNk6cG/hHgd4B/kzT3380fAZ8D7pO0FTgMfKKZLNajDw/3LdNxN6rDtq7pUMd9tsqTkfLWXscd+6LfybR08tac66zJV7li6LJG1+WdrlVq5dNU+T6LTBDWxbj9PKNQ/hnQmNVX1ZsdMzPLy7fSm5klShHR2s6WallcoWKV9rrnMh7WRAfYKE1OxpT3krvIpXneMfFVLveLNCfVNTlUXcenbtNupR8llY7PuvStQ3OUJptQHo4dT0TEuvnLXQM3M0tUryazmqSJGnJbQ97qrj2UqU3XNblPn58bOO3YTzoWVSctaqKjrO67SftWY52m6yF6eXSdR9fAzcwS5QLczCxRyTShVO3gqyvNaelXuSNvlDaeFDRun1UvD5toVpi/volL2DJNR30Ys93XZoZx+nZMU+QauJlZolyAm5klqtUmlPMuPsGuXcVur65rzPKo7eueg7isos0BRY5JlXSK6GrMcpG0844DL5t+3fo84qeKvj4qroi+xOAauJlZolqtgT/z5JLCNcImO/uqjnPuwzjdujtNi+yniX3WXevMe0ybrknVPS3ttM/0TdfjpWeVa+BmZolyAW5mlqjOx4H3YTz0cPpFLmHLTPpUZv0kZTqD67pMb+I76dvldZWmjzLb9S3+qvrS2VeHPjYDuQZuZpaoVqeTXXfJGfHYrtVj19c9EVTVjrcqE0VVrbGWqbkUfUpIH2sUfTDt2YfTdHUsm7hztupvbVbPq7avLDydrJnZjHEBbmaWqFabUCQdB14FXmhtp81bzuzEM0uxwGzFM0uxwGzF00Ys74+Is+YvbLUAB5C0d1RbTqpmKZ5ZigVmK55ZigVmK54uY3ETiplZolyAm5klqosCfHsH+2zSLMUzS7HAbMUzS7HAbMXTWSytt4GbmVk93IRiZpaoVgtwSRskHZB0SNK2NvddlaTVkr4tab+kfZJuyJYvk7Rb0sHs75ld5zUvSYsk/aukB7P3Kcfybkk7JD2dfUe/mHg8N2bn2VOS7pF0RirxSLpd0jFJTw0tG5t3STdnZcIBSdd0k+vxxsTz+exce1LSP0p699C61uJprQCXtAj4K+CjwIXAZkkXtrX/GpwEPhMRFwBXAp/K8r8N2BMRa4E92ftU3ADsH3qfciy3Ad+MiA8BlzCIK8l4JK0E/gBYFxEXAYuATaQTzx3AhnnLRuY9+w1tAj6cfeavs7KiT+7grfHsBi6KiIuBZ4Cbof142qyBXw4ciojvR8TrwL3Axhb3X0lEHI2If8lev8yggFjJIIY7s83uBH6zmxwWI2kV8BvAl4YWpxrLUuBXgC8DRMTrEfEiicaTWQy8Q9JiYAnwHInEExH/BPxk3uJxed8I3BsRr0XED4BDDMqK3hgVT0Q8FBEns7ePAKuy163G02YBvhJ4duj9kWxZciStAS4DHgXOiYijMCjkgbO7y1khfwn8IfDm0LJUY/l54DjwlaxJ6EuS3kmi8UTEj4AvAIeBo8B/R8RDJBpPZlzeZ6Fc+F3gG9nrVuNpswDXiGXJDYGR9C7gq8CnI+KlrvNThqRrgWMR8UTXeanJYuAXgL+JiMsYTNfQ1+aFqbL24Y3AucB7gXdKuq7bXDUm6XJB0i0Mmlfvmls0YrPG4mmzAD8CDM8lu4rBZWEyJJ3GoPC+KyLuzxY/L2lFtn4FcKyr/BXwEeDjkn7IoCnr1yT9PWnGAoNz60hEPJq938GgQE81nquBH0TE8Yh4A7gf+CXSjQfG5z3ZckHSFuBa4JPx/+OxW42nzQL8cWCtpHMlvZ1BQ//OFvdfiSQxaGPdHxG3Dq3aCWzJXm8BHmg7b0VFxM0RsSoi1jD4Hr4VEdeRYCwAEfFj4FlJ52eLrgK+R6LxMGg6uVLSkuy8u4pBn0uq8cD4vO8ENkk6XdK5wFrgsQ7yV4ikDcBNwMcj4sTQqnbjiYjW/gEfY9Bj++/ALW3uu4a8/zKDS6Enge9m/z4GvIdBr/rB7O+yrvNaMK71wIPZ62RjAS4F9mbfz9eAMxOP50+Bp4GngL8DTk8lHuAeBm33bzCokW6dlHfglqxMOAB8tOv854znEIO27rmy4ItdxOM7Mc3MEuU7Mc3MEuUC3MwsUS7AzcwS5QLczCxRLsDNzBLlAtzMLFEuwM3MEuUC3MwsUf8LIxgQbEJMsxAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "danube_gridpoints = get_mask_of_basin(glofas['dis'].isel(time=0), 'Danube')\n",
    "plt.imshow(danube_gridpoints.astype(int))\n",
    "plt.show()\n",
    "\n",
    "#mask_springs = glofas['dis'].isel(time=0)\n",
    "#mask_springs.values[:] = 0.\n",
    "\n",
    "dis_map_mean = glofas['dis'].mean('time')\n",
    "is_river = select_riverpoints(dis_map_mean)\n",
    "\n",
    "plt.imshow(is_river.astype(int))\n",
    "plt.title('is_river')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = []\n",
    "\n",
    "for lon in danube_gridpoints.longitude:\n",
    "    for lat in danube_gridpoints.latitude:\n",
    "        #print(danube_gridpoints.sel(latitude=lat, longitude=lon))\n",
    "        if danube_gridpoints.sel(latitude=lat, longitude=lon) == 1:\n",
    "            lat, lon = float(lat), float(lon)\n",
    "            task_list.append(train_flowmodel(lat, lon))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2482"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(task_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with ProgressBar():\n",
    "    dask.compute(task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=20)]: Using backend LokyBackend with 20 concurrent workers.\n",
      "[Parallel(n_jobs=20)]: Done   1 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=20)]: Done  10 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=20)]: Done  21 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=20)]: Done  32 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=20)]: Done  45 tasks      | elapsed:   15.7s\n",
      "[Parallel(n_jobs=20)]: Done  58 tasks      | elapsed:   16.2s\n",
      "[Parallel(n_jobs=20)]: Done  73 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=20)]: Done  88 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=20)]: Done 105 tasks      | elapsed:   18.1s\n",
      "[Parallel(n_jobs=20)]: Done 122 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=20)]: Done 141 tasks      | elapsed:   19.1s\n",
      "[Parallel(n_jobs=20)]: Done 160 tasks      | elapsed:   19.6s\n",
      "[Parallel(n_jobs=20)]: Done 181 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=20)]: Done 202 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=20)]: Done 225 tasks      | elapsed:  7.8min\n",
      "[Parallel(n_jobs=20)]: Done 248 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=20)]: Done 273 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=20)]: Done 298 tasks      | elapsed: 13.4min\n",
      "[Parallel(n_jobs=20)]: Done 325 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=20)]: Done 352 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=20)]: Done 381 tasks      | elapsed: 16.1min\n",
      "[Parallel(n_jobs=20)]: Done 410 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=20)]: Done 441 tasks      | elapsed: 18.1min\n",
      "[Parallel(n_jobs=20)]: Done 472 tasks      | elapsed: 19.0min\n",
      "[Parallel(n_jobs=20)]: Done 505 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=20)]: Done 538 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=20)]: Done 573 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=20)]: Done 608 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=20)]: Done 645 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=20)]: Done 682 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=20)]: Done 721 tasks      | elapsed: 27.4min\n",
      "[Parallel(n_jobs=20)]: Done 760 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=20)]: Done 801 tasks      | elapsed: 30.1min\n",
      "[Parallel(n_jobs=20)]: Done 842 tasks      | elapsed: 31.6min\n",
      "[Parallel(n_jobs=20)]: Done 885 tasks      | elapsed: 32.9min\n",
      "[Parallel(n_jobs=20)]: Done 928 tasks      | elapsed: 34.1min\n",
      "[Parallel(n_jobs=20)]: Done 973 tasks      | elapsed: 35.2min\n",
      "[Parallel(n_jobs=20)]: Done 1018 tasks      | elapsed: 36.7min\n",
      "[Parallel(n_jobs=20)]: Done 1065 tasks      | elapsed: 38.1min\n",
      "[Parallel(n_jobs=20)]: Done 1112 tasks      | elapsed: 39.5min\n",
      "[Parallel(n_jobs=20)]: Done 1161 tasks      | elapsed: 40.7min\n",
      "[Parallel(n_jobs=20)]: Done 1210 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=20)]: Done 1261 tasks      | elapsed: 43.0min\n",
      "[Parallel(n_jobs=20)]: Done 1312 tasks      | elapsed: 44.3min\n",
      "[Parallel(n_jobs=20)]: Done 1365 tasks      | elapsed: 45.5min\n",
      "[Parallel(n_jobs=20)]: Done 1418 tasks      | elapsed: 46.7min\n",
      "[Parallel(n_jobs=20)]: Done 1473 tasks      | elapsed: 48.0min\n",
      "[Parallel(n_jobs=20)]: Done 1528 tasks      | elapsed: 49.4min\n",
      "[Parallel(n_jobs=20)]: Done 1585 tasks      | elapsed: 50.7min\n",
      "[Parallel(n_jobs=20)]: Done 1642 tasks      | elapsed: 52.1min\n",
      "[Parallel(n_jobs=20)]: Done 1701 tasks      | elapsed: 53.7min\n",
      "[Parallel(n_jobs=20)]: Done 1760 tasks      | elapsed: 55.3min\n",
      "[Parallel(n_jobs=20)]: Done 1821 tasks      | elapsed: 57.0min\n",
      "[Parallel(n_jobs=20)]: Done 1882 tasks      | elapsed: 59.2min\n",
      "[Parallel(n_jobs=20)]: Done 1945 tasks      | elapsed: 62.0min\n",
      "[Parallel(n_jobs=20)]: Done 2008 tasks      | elapsed: 64.0min\n",
      "[Parallel(n_jobs=20)]: Done 2073 tasks      | elapsed: 65.7min\n",
      "[Parallel(n_jobs=20)]: Done 2138 tasks      | elapsed: 67.4min\n",
      "[Parallel(n_jobs=20)]: Done 2205 tasks      | elapsed: 69.1min\n",
      "[Parallel(n_jobs=20)]: Done 2272 tasks      | elapsed: 71.0min\n",
      "[Parallel(n_jobs=20)]: Done 2341 tasks      | elapsed: 72.7min\n",
      "[Parallel(n_jobs=20)]: Done 2410 tasks      | elapsed: 74.2min\n",
      "[Parallel(n_jobs=20)]: Done 2482 out of 2482 | elapsed: 75.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parallel(n_jobs=20, verbose=10)(task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
