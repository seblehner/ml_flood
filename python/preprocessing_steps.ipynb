{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as np\n",
    "import seaborn as sb\n",
    "import xarray as xr\n",
    "import os.path as path\n",
    "from aux.utils import open_data\n",
    "from aux.ml_flood_config import path_to_data\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "client = Client(processes=False)  #memory_limit='16GB', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/srvx11/lehre/users/a1303583/ipython/ml_flood/data/danube/monthly_files/\n"
     ]
    }
   ],
   "source": [
    "# load dask client\n",
    "client\n",
    "# define some vars\n",
    "data_path = f'{path_to_data}danube/monthly_files/'\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rename files if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux.utils import rename_files\n",
    "\n",
    "#rename_files(path=data_path, old='day.', new='dayavg.', str_constraint='temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert hourly files to daily means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from aux.utils import cdo_daily_means\n",
    "\n",
    "#cdo_daily_means(path=data_path, file_includes='single_level')\n",
    "#cdo_daily_means(path=data_path, file_includes='850_700_500')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert precipitation values to daily sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from aux.utils import cdo_precip_sums\n",
    "\n",
    "#cdo_precip_sums(path=data_path, file_includes='large_scale_precipitation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract precipitation parameters from daily sums and delete them from daily averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux.utils import cdo_clean_precip\n",
    "\n",
    "#cdo_clean_precip(path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge files (time dim) together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux.utils import cdo_merge_time\n",
    "\n",
    "# danube\n",
    "#cdo_merge_time(path=data_path, file_includes='large_scale_precipitation', new_file='era5_lsp_1981-2017_daysum.nc')\n",
    "#cdo_merge_time(path=data_path, file_includes='convective_precipitation', new_file='era5_tp_cp_1981-2017_daysum.nc')\n",
    "#cdo_merge_time(path=data_path, file_includes='_temperature_', new_file='era5_z_t_q_1981-2017_dayavg.nc')\n",
    "#cdo_merge_time(path=data_path, file_includes='soil_water', new_file='era5_swvl1_swvl2_ro_tcwv_1981-2017_dayavg.nc')\n",
    "\n",
    "# usa\n",
    "#cdo_merge_time(path=data_path, file_includes='geopotential,temperature', new_file='era5_z_t_q_1981-2017_dayavg.nc')\n",
    "#cdo_merge_time(path=data_path, file_includes='convective_precipitation', new_file='era5_lsp_cp_1981-2017_daysum.nc')\n",
    "#cdo_merge_time(path=data_path, file_includes='runoff', new_file='era5_ro_1981-2017_daysum.nc')\n",
    "#cdo_merge_time(path=data_path, file_includes='soil_water', new_file='era5_swvl1_swvl2_tcwv_1981-2017_dayavg.nc')\n",
    "\n",
    "# asia\n",
    "#cdo_merge_time(path=data_path, file_includes='geopotential,temperature', new_file='era5_z_t_q_1981-2017_dayavg.nc')\n",
    "#cdo_merge_time(path=data_path, file_includes='convective_precipitation', new_file='era5_lsp_cp_1981-2017_daysum.nc')\n",
    "#cdo_merge_time(path=data_path, file_includes='runoff', new_file='era5_ro_1981-2017_daysum.nc')\n",
    "#cdo_merge_time(path=data_path, file_includes='soil_water', new_file='era5_swvl1_swvl2_tcwv_1981-2017_dayavg.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOFAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1981-2002 done; 2003-2018 done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_to_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ee474b4b6d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{path_to_data}glofas/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'glofas'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_to_data' is not defined"
     ]
    }
   ],
   "source": [
    "# extract data from tar files\n",
    "import os\n",
    "\n",
    "data_path = f'{path_to_data}glofas/'\n",
    "for name in os.listdir(data_path):\n",
    "    if 'glofas' in name:\n",
    "        print(f'extracting data from {name} ...')\n",
    "        file = data_path+name\n",
    "        #os.system(f'tar -xvf {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_to_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f712fced14f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdo_merge_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# define some vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{path_to_data}glofas/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#!ls /home/srvx11/lehre/users/a1303583/ipython/ml_flood/data/glofas/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_to_data' is not defined"
     ]
    }
   ],
   "source": [
    "from aux.utils import cdo_spatial_cut\n",
    "from aux.utils import cdo_merge_time\n",
    "# define some vars\n",
    "data_path = f'{path_to_data}glofas/'\n",
    "print(data_path)\n",
    "#!ls /home/srvx11/lehre/users/a1303583/ipython/ml_flood/data/glofas/\n",
    "\n",
    "# DANUBE\n",
    "# cut out spatial region\n",
    "#cdo_spatial_cut(path=data_path, file_includes='dis_', new_file_includes='danube', lonmin=7, lonmax=20, latmin=47, latmax=50)\n",
    "# merge into one file\n",
    "#cdo_merge_time(path=data_path, file_includes='spatial_cut_danube', new_file='glofas_reanalysis_danube_2003-2018.nc')\n",
    "\n",
    "# USA\n",
    "#cdo_spatial_cut(path=data_path, file_includes='dis_', new_file_includes='usa', lonmin=-125, lonmax=-70, latmin=25, latmax=50)\n",
    "#cdo_merge_time(path=data_path, file_includes='spatial_cut_usa', new_file='glofas_reanalysis_usa_2003-2018.nc')\n",
    "\n",
    "# ASIA\n",
    "#cdo_spatial_cut(path=data_path, file_includes='dis_', new_file_includes='asia', lonmin=35, lonmax=140, latmin=0, latmax=55)\n",
    "#cdo_merge_time(path=data_path, file_includes='spatial_cut_asia', new_file='glofas_reanalysis_asia_2003-2018.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data in a generalized form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(path, kw='era5'):\n",
    "    \"\"\"\n",
    "    Opens all available ERA5/glofas datasets (depending on the keyword) in the specified path and resamples time to match\n",
    "    the timestamp /per day (through the use of cdo YYYYMMDD 23z is the corresponding time\n",
    "    stamp) in the case of era5, or renames lat lon in the case of glofas.\n",
    "    \"\"\"\n",
    "    if kw is 'era5':    \n",
    "        ds = xr.open_mfdataset(data_path+'*era5*')\n",
    "        ds.coords['time'] = pd.to_datetime(ds.coords['time'].values) - datetime.timedelta(hours=23)\n",
    "    elif kw is 'glofas_ra':\n",
    "        ds = xr.open_mfdataset(data_path+'*glofas_reanalysis*')\n",
    "        ds = ds.rename({'lat': 'latitude', 'lon': 'longitude'})\n",
    "    elif kw is 'glofas_fr':\n",
    "        ds = xr.open_mfdataset(data_path+'*glofas_forecast*')\n",
    "        ds = ds.rename({'lat': 'latitude', 'lon': 'longitude'})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "era5 = open_data(data_path, kw='era5')\n",
    "glofas = open_data(data_path, kw='glofas_ra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (latitude: 13, level: 3, longitude: 53, time: 13514)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2017-12-31\n",
      "  * longitude  (longitude) float32 7.0 7.25 7.5 7.75 ... 19.25 19.5 19.75 20.0\n",
      "  * latitude   (latitude) float32 50.0 49.75 49.5 49.25 ... 47.5 47.25 47.0\n",
      "  * level      (level) float64 850.0 700.0 500.0\n",
      "Data variables:\n",
      "    cp         (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "    tp         (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "    slt        (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "    z          (time, latitude, longitude, level) float32 dask.array<shape=(13514, 13, 53, 3), chunksize=(13514, 13, 53, 3)>\n",
      "    slor       (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "    lsm        (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "    lsp        (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "    t          (time, level, latitude, longitude) float32 dask.array<shape=(13514, 3, 13, 53), chunksize=(13514, 3, 13, 53)>\n",
      "    q          (time, level, latitude, longitude) float32 dask.array<shape=(13514, 3, 13, 53), chunksize=(13514, 3, 13, 53)>\n",
      "    ro         (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "    swvl1      (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "    swvl2      (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "    tcwv       (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(13514, 13, 53)>\n",
      "Attributes:\n",
      "    CDI:          Climate Data Interface version 1.6.4rc4 (http://code.zmaw.d...\n",
      "    Conventions:  CF-1.6\n",
      "    history:      Fri May 31 17:03:42 2019: cdo mergetime reanalysis-era5-sin...\n",
      "    CDO:          Climate Data Operators version 1.6.4rc7 (http://code.zmaw.d...\n",
      "<xarray.Dataset>\n",
      "Dimensions:    (latitude: 30, longitude: 130, time: 13879)\n",
      "Coordinates:\n",
      "  * longitude  (longitude) float64 7.05 7.15 7.25 7.35 ... 19.75 19.85 19.95\n",
      "  * latitude   (latitude) float64 49.95 49.85 49.75 49.65 ... 47.25 47.15 47.05\n",
      "  * time       (time) datetime64[ns] 1981-01-02 1981-01-03 ... 2019-01-01\n",
      "Data variables:\n",
      "    dis        (time, latitude, longitude) float32 dask.array<shape=(13879, 30, 130), chunksize=(8035, 30, 130)>\n",
      "Attributes:\n",
      "    CDI:          Climate Data Interface version 1.6.4rc4 (http://code.zmaw.d...\n",
      "    Conventions:  CF-1.4\n",
      "    history:      Thu Jun 13 19:25:04 2019: cdo mergetime /home/srvx11/lehre/...\n",
      "    CDO:          Climate Data Operators version 1.6.4rc7 (http://code.zmaw.d...\n"
     ]
    }
   ],
   "source": [
    "print(era5)\n",
    "print(glofas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load all data from monthly files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (latitude: 13, level: 3, longitude: 53, time: 13514)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 1981-01-01 1981-01-02 ... 2017-12-31\n",
      "  * latitude   (latitude) float32 50.0 49.75 49.5 49.25 ... 47.5 47.25 47.0\n",
      "  * longitude  (longitude) float32 7.0 7.25 7.5 7.75 ... 19.25 19.5 19.75 20.0\n",
      "  * level      (level) float64 850.0 700.0 500.0\n",
      "Data variables:\n",
      "    cp         (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(31, 13, 53)>\n",
      "    tp         (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(31, 13, 53)>\n",
      "    lsp        (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(31, 13, 53)>\n",
      "    z          (time, level, latitude, longitude) float32 dask.array<shape=(13514, 3, 13, 53), chunksize=(31, 3, 13, 53)>\n",
      "    t          (time, level, latitude, longitude) float32 dask.array<shape=(13514, 3, 13, 53), chunksize=(31, 3, 13, 53)>\n",
      "    q          (time, level, latitude, longitude) float32 dask.array<shape=(13514, 3, 13, 53), chunksize=(31, 3, 13, 53)>\n",
      "    ro         (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(31, 13, 53)>\n",
      "    swvl1      (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(31, 13, 53)>\n",
      "    swvl2      (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(31, 13, 53)>\n",
      "    tcwv       (time, latitude, longitude) float32 dask.array<shape=(13514, 13, 53), chunksize=(31, 13, 53)>\n",
      "Attributes:\n",
      "    CDI:          Climate Data Interface version 1.6.4rc4 (http://code.zmaw.d...\n",
      "    Conventions:  CF-1.6\n",
      "    history:      Thu May 30 12:35:16 2019: cdo dayavg /home/srvx11/lehre/use...\n",
      "    CDO:          Climate Data Operators version 1.6.4rc7 (http://code.zmaw.d...\n"
     ]
    }
   ],
   "source": [
    "#era5 = open_data(data_path, kw='era5')\n",
    "print(era5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "era5.chunk(chunks=dict(time=-1)).to_netcdf('era5_danube_pressure_and_single_levels.nc', engine='netcdf4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
